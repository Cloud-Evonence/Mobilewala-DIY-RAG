{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19cf19d7-c64b-4e2b-8407-377003ed7ea1",
      "metadata": {
        "tags": [],
        "id": "19cf19d7-c64b-4e2b-8407-377003ed7ea1",
        "outputId": "caffc09c-bc4c-4678-e99f-87e18555c425"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Downloading merged_dataset.csv from GCS to /tmp/merged_dataset_1.csv\n",
            "INFO:__main__:Columns in /tmp/merged_dataset_1.csv: ['id', 'Human_Readable_Summary']\n",
            "INFO:__main__:First few rows:\n",
            "   id                             Human_Readable_Summary\n",
            "0   1  During the period of 2024 June to 2024 July, i...\n",
            "1   2  During the period of 2024 June to 2024 July, i...\n",
            "2   3  During the period of 2024 June to 2024 July, i...\n",
            "3   4  During the period of 2024 June to 2024 July, i...\n",
            "4   5  During the period of 2024 June to 2024 July, i...\n",
            "INFO:__main__:Initializing textembedding-gecko@003 model\n",
            "INFO:__main__:Regenerating embeddings.\n",
            "INFO:__main__:Processing chunk 1 with 0 rows\n",
            "INFO:__main__:Chunk columns: ['id', 'Human_Readable_Summary']\n",
            "INFO:__main__:Total rows processed so far: 625671\n",
            "INFO:__main__:Step 1 complete: Embeddings generated and saved to local CSV.\n",
            "INFO:__main__:Uploaded /tmp/embeddings.csv to gs://mw-llm-poc-dataproc/embeddings.csv for persistence\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from google.api_core import retry\n",
        "from google.api_core import exceptions\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# GCS client\n",
        "storage_client = storage.Client()\n",
        "bucket_name = \"mw-llm-poc-dataproc\"\n",
        "file_path = \"merged_dataset.csv\"\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "\n",
        "# Local paths\n",
        "local_file = \"/tmp/merged_dataset_1.csv\"\n",
        "embeddings_csv = \"/tmp/embeddings.csv\"\n",
        "checkpoint_file = \"/tmp/checkpoint.json\"\n",
        "\n",
        "# Configurable summary column name\n",
        "SUMMARY_COLUMN = \"Human_Readable_Summary\"\n",
        "\n",
        "# Download file locally\n",
        "logger.info(f\"Downloading {file_path} from GCS to {local_file}\")\n",
        "blob.download_to_filename(local_file)\n",
        "\n",
        "# Debug: Check column names\n",
        "try:\n",
        "    df = pd.read_csv(local_file, nrows=5)\n",
        "    logger.info(f\"Columns in {local_file}: {df.columns.tolist()}\")\n",
        "    logger.info(f\"First few rows:\\n{df.head()}\")\n",
        "    if SUMMARY_COLUMN not in df.columns:\n",
        "        logger.error(f\"Column '{SUMMARY_COLUMN}' not found. Available columns: {df.columns.tolist()}\")\n",
        "        raise KeyError(f\"Column '{SUMMARY_COLUMN}' not found in {local_file}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error reading {local_file}: {e}\")\n",
        "    raise\n",
        "\n",
        "# Initialize the embedding model\n",
        "logger.info(\"Initializing textembedding-gecko@003 model\")\n",
        "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
        "\n",
        "# Function to generate embeddings\n",
        "def get_embeddings(texts):\n",
        "    try:\n",
        "        embeddings = model.get_embeddings(texts)\n",
        "        return [embedding.values for embedding in embeddings]\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating embeddings: {e}\")\n",
        "        return None\n",
        "\n",
        "# Checkpoint management\n",
        "def load_checkpoint():\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    checkpoint_blob = bucket.blob(\"checkpoint.json\")\n",
        "    if checkpoint_blob.exists():\n",
        "        checkpoint_blob.download_to_filename(checkpoint_file)\n",
        "        with open(checkpoint_file, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    return {\n",
        "        \"step1_rows_processed\": 0,\n",
        "        \"step1_complete\": False,\n",
        "        \"step2_complete\": False,\n",
        "        \"step3_complete\": False,\n",
        "        \"step4_complete\": False,\n",
        "        \"step5_complete\": False\n",
        "    }\n",
        "\n",
        "def save_checkpoint(rows_processed, step1_complete=False, step2_complete=False, step3_complete=False, step4_complete=False, step5_complete=False):\n",
        "    checkpoint = load_checkpoint()\n",
        "    checkpoint[\"step1_rows_processed\"] = rows_processed\n",
        "    checkpoint[\"step1_complete\"] = step1_complete\n",
        "    checkpoint[\"step2_complete\"] = step2_complete\n",
        "    checkpoint[\"step3_complete\"] = step3_complete\n",
        "    checkpoint[\"step4_complete\"] = step4_complete\n",
        "    checkpoint[\"step5_complete\"] = step5_complete\n",
        "    with open(checkpoint_file, \"w\") as f:\n",
        "        json.dump(checkpoint, f)\n",
        "    @retry.Retry(predicate=retry.if_exception_type(exceptions.TooManyRequests))\n",
        "    def upload_with_retry():\n",
        "        checkpoint_blob = bucket.blob(\"checkpoint.json\")\n",
        "        checkpoint_blob.upload_from_filename(checkpoint_file)\n",
        "    try:\n",
        "        upload_with_retry()\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to upload checkpoint to GCS: {e}. Continuing with local checkpoint.\")\n",
        "\n",
        "# Step 1: Generate embeddings\n",
        "checkpoint = load_checkpoint()\n",
        "rows_processed = checkpoint[\"step1_rows_processed\"]\n",
        "total_rows_processed = rows_processed  # Initialize total_rows_processed\n",
        "chunk_size = 10000\n",
        "batch_size = 50  # Token limit safe\n",
        "\n",
        "# Check if embeddings file exists locally or in GCS\n",
        "embeddings_exists_locally = os.path.exists(embeddings_csv)\n",
        "embeddings_blob = bucket.blob(\"embeddings.csv\")\n",
        "embeddings_exists_in_gcs = embeddings_blob.exists()\n",
        "\n",
        "# If Step 1 is marked as complete but the embeddings file is missing, reset the checkpoint\n",
        "if checkpoint.get(\"step1_complete\", False) and not (embeddings_exists_locally or embeddings_exists_in_gcs):\n",
        "    logger.warning(\"Step 1 marked as complete, but embeddings file is missing both locally and in GCS. Resetting Step 1 checkpoint.\")\n",
        "    checkpoint[\"step1_complete\"] = False\n",
        "    checkpoint[\"step1_rows_processed\"] = 0\n",
        "    rows_processed = 0\n",
        "    total_rows_processed = 0\n",
        "    with open(checkpoint_file, \"w\") as f:\n",
        "        json.dump(checkpoint, f)\n",
        "\n",
        "if checkpoint.get(\"step1_complete\", False):\n",
        "    logger.info(\"Step 1 already complete for all 630,000 rows.\")\n",
        "    if not embeddings_exists_locally and embeddings_exists_in_gcs:\n",
        "        logger.info(f\"Downloading {embeddings_csv} from GCS since it’s missing locally\")\n",
        "        embeddings_blob.download_to_filename(embeddings_csv)\n",
        "else:\n",
        "    logger.info(\"Regenerating embeddings.\")\n",
        "    mode = \"a\" if rows_processed > 0 else \"w\"\n",
        "    with open(embeddings_csv, mode, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\")\n",
        "        if rows_processed == 0:\n",
        "            writer.writerow([\"id\", SUMMARY_COLUMN, \"embedding\"])\n",
        "\n",
        "        skiprows = range(1, rows_processed + 1) if rows_processed > 0 else None\n",
        "        data_chunks = pd.read_csv(local_file, chunksize=chunk_size, skiprows=skiprows, header=0)\n",
        "        total_rows_processed = rows_processed\n",
        "        for chunk_idx, chunk in enumerate(data_chunks):\n",
        "            logger.info(f\"Processing chunk {chunk_idx + 1} with {len(chunk)} rows\")\n",
        "            logger.info(f\"Chunk columns: {chunk.columns.tolist()}\")\n",
        "            if SUMMARY_COLUMN not in chunk.columns:\n",
        "                logger.error(f\"Column '{SUMMARY_COLUMN}' not found in chunk. Available columns: {chunk.columns.tolist()}\")\n",
        "                raise KeyError(f\"Column '{SUMMARY_COLUMN}' not found in chunk\")\n",
        "\n",
        "            batch_summaries = []\n",
        "            batch_ids = []\n",
        "            for idx, row in chunk.iterrows():\n",
        "                summary = str(row[SUMMARY_COLUMN])\n",
        "                batch_summaries.append(summary)\n",
        "                batch_ids.append(row[\"id\"])\n",
        "                if len(batch_summaries) == batch_size:\n",
        "                    embeddings = get_embeddings(batch_summaries)\n",
        "                    if embeddings:\n",
        "                        for id_val, summary, embedding in zip(batch_ids, batch_summaries, embeddings):\n",
        "                            embedding_str = ','.join(map(str, embedding))\n",
        "                            writer.writerow([id_val, summary, embedding_str])\n",
        "                        total_rows_processed += len(batch_summaries)\n",
        "                    else:\n",
        "                        logger.warning(f\"Skipping batch due to embedding error\")\n",
        "                        for id_val, summary in zip(batch_ids, batch_summaries):\n",
        "                            writer.writerow([id_val, summary, \"\"])\n",
        "                        total_rows_processed += len(batch_summaries)\n",
        "                    batch_summaries = []\n",
        "                    batch_ids = []\n",
        "\n",
        "            if batch_summaries:\n",
        "                embeddings = get_embeddings(batch_summaries)\n",
        "                if embeddings:\n",
        "                    for id_val, summary, embedding in zip(batch_ids, batch_summaries, embeddings):\n",
        "                        embedding_str = ','.join(map(str, embedding))\n",
        "                        writer.writerow([id_val, summary, embedding_str])\n",
        "                    total_rows_processed += len(batch_summaries)\n",
        "                else:\n",
        "                    logger.warning(f\"Skipping remaining batch due to embedding error\")\n",
        "                    for id_val, summary in zip(batch_ids, batch_summaries):\n",
        "                        writer.writerow([id_val, summary, \"\"])\n",
        "                    total_rows_processed += len(batch_summaries)\n",
        "\n",
        "            save_checkpoint(total_rows_processed, step1_complete=False)\n",
        "            logger.info(f\"Total rows processed so far: {total_rows_processed}\")\n",
        "            if total_rows_processed >= 630000:\n",
        "                break\n",
        "        save_checkpoint(total_rows_processed, step1_complete=True)\n",
        "    logger.info(\"Step 1 complete: Embeddings generated and saved to local CSV.\")\n",
        "    # Upload embeddings to GCS for persistence\n",
        "    embeddings_blob = bucket.blob(\"embeddings.csv\")\n",
        "    embeddings_blob.upload_from_filename(embeddings_csv)\n",
        "    logger.info(f\"Uploaded {embeddings_csv} to gs://{bucket_name}/embeddings.csv for persistence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b1ca73-2d68-49df-b0e7-7d693b06cd88",
      "metadata": {
        "tags": [],
        "id": "29b1ca73-2d68-49df-b0e7-7d693b06cd88",
        "outputId": "87bdaf22-fed1-496d-e828-fb7071422e42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Step 2 already complete based on checkpoint.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Load embeddings into BigQuery\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client()\n",
        "project_id = \"856598595188\"\n",
        "dataset_id = \"mw_llm_poc\"\n",
        "table_id = \"embeddings_final_table\"\n",
        "table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "if checkpoint.get(\"step2_complete\", False):\n",
        "    logger.info(\"Step 2 already complete based on checkpoint.\")\n",
        "else:\n",
        "    # Ensure embeddings file exists before proceeding\n",
        "    if not os.path.exists(embeddings_csv):\n",
        "        if embeddings_blob.exists():\n",
        "            logger.info(f\"Downloading {embeddings_csv} from GCS since it’s missing locally\")\n",
        "            embeddings_blob.download_to_filename(embeddings_csv)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Embeddings file {embeddings_csv} not found locally or in GCS. Cannot proceed with Step 2.\")\n",
        "\n",
        "    logger.info(f\"Creating dataset {dataset_id} if it doesn’t exist\")\n",
        "    dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
        "    dataset = client.create_dataset(dataset, exists_ok=True)\n",
        "\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"id\", \"INTEGER\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(SUMMARY_COLUMN, \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"embedding\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    ]\n",
        "    logger.info(f\"Creating table {table_id}\")\n",
        "    table = bigquery.Table(table_ref, schema=schema)\n",
        "    table = client.create_table(table, exists_ok=True)\n",
        "\n",
        "    gcs_path = f\"gs://{bucket_name}/embeddings.csv\"\n",
        "    logger.info(f\"Uploading {embeddings_csv} to {gcs_path}\")\n",
        "    blob = bucket.blob(\"embeddings.csv\")\n",
        "    blob.upload_from_filename(embeddings_csv)\n",
        "\n",
        "    logger.info(f\"Loading {gcs_path} into {table_ref}\")\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        schema=schema,\n",
        "        write_disposition=\"WRITE_TRUNCATE\",\n",
        "        allow_quoted_newlines=True,\n",
        "    )\n",
        "    load_job = client.load_table_from_uri(gcs_path, table_ref, job_config=job_config)\n",
        "    load_job.result()\n",
        "    if load_job.errors:\n",
        "        logger.error(f\"Load job failed: {load_job.errors}\")\n",
        "        raise Exception(f\"Load job failed: {load_job.errors}\")\n",
        "    else:\n",
        "        logger.info(\"Embeddings loaded into BigQuery.\")\n",
        "\n",
        "    logger.info(\"Converting embedding column to ARRAY<FLOAT64>\")\n",
        "    query = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE `{table_ref}` AS\n",
        "        SELECT\n",
        "            id,\n",
        "            {SUMMARY_COLUMN},\n",
        "            CASE\n",
        "                WHEN embedding IS NOT NULL AND embedding != ''\n",
        "                THEN SPLIT(embedding, ',')\n",
        "                ELSE NULL\n",
        "            END AS embedding\n",
        "        FROM `{table_ref}`\n",
        "    \"\"\"\n",
        "    query_job = client.query(query)\n",
        "    query_job.result()\n",
        "    save_checkpoint(total_rows_processed, step2_complete=True)\n",
        "    logger.info(\"Step 2 complete: Embeddings stored in BigQuery.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f8ab073-6bbe-4bfd-bcc9-d0d4a3060fba",
      "metadata": {
        "tags": [],
        "id": "0f8ab073-6bbe-4bfd-bcc9-d0d4a3060fba",
        "outputId": "7865f42e-ed5e-4737-b8eb-0aaee3829e63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Generated query embedding with length: 768\n",
            "INFO:__main__:Executing similarity query: If Comcast had served 2.5 million households instead, what would its market share have been?\n",
            "INFO:__main__:Retrieved 5 summaries\n",
            "INFO:__main__:Step 3 complete: Retrieval implemented.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 202992, Summary: In the USA, during 2024, July, Comcast providing Fixed Broadband services served 38,458,487 households out of a total of 136,378,955 in the country, resulting in a market share of 28.2%.\n",
            "ID: 201822, Summary: In the state of North Carolina (NC) in USA, during 2024, July, Comcast providing Fixed Broadband services served 19,418 households out of a total of 4,780,848 in the country, resulting in a market share of 0.41%.\n",
            "ID: 201414, Summary: In the state of Maryland (MD) in USA, during 2024, July, Comcast providing Fixed Broadband services served 1,532,831 households out of a total of 2,812,665 in the country, resulting in a market share of 54.50%.\n",
            "ID: 201558, Summary: In the state of New York (NY) in USA, during 2024, July, Comcast providing Fixed Broadband services served 111,597 households out of a total of 7,137,002 in the country, resulting in a market share of 1.56%.\n",
            "ID: 201392, Summary: In the state of California (CA) in USA, during 2024, July, Comcast providing Fixed Broadband services served 3,772,423 households out of a total of 14,411,291 in the country, resulting in a market share of 26.18%.\n"
          ]
        }
      ],
      "source": [
        "#step3: Retrieving Similar Summaries\n",
        "def retrieve_similar_summaries(query_text, top_k=5):\n",
        "    query_embedding = get_embeddings([query_text])\n",
        "    if query_embedding is None:\n",
        "        logger.error(\"Failed to generate embedding for query: %s\", query_text)\n",
        "        return []\n",
        "\n",
        "    query_embedding_str = ', '.join(map(str, query_embedding[0]))\n",
        "    logger.info(\"Generated query embedding with length: %d\", len(query_embedding[0]))\n",
        "\n",
        "    similarity_query = f\"\"\"\n",
        "        SELECT\n",
        "            t.id,\n",
        "            t.{SUMMARY_COLUMN},\n",
        "            SUM(CAST(t.embedding[offset(i)] AS FLOAT64) * q_emb) AS dot_product\n",
        "        FROM `{table_ref}` t\n",
        "        CROSS JOIN UNNEST([{query_embedding_str}]) AS q_emb WITH OFFSET i\n",
        "        WHERE ARRAY_LENGTH(t.embedding) = {len(query_embedding[0])}\n",
        "        GROUP BY t.id, t.{SUMMARY_COLUMN}\n",
        "        ORDER BY dot_product DESC\n",
        "        LIMIT {top_k}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Executing similarity query: %s\", query_text)\n",
        "        query_job = client.query(similarity_query)\n",
        "        results = query_job.result()\n",
        "        retrieved = [{\"id\": row[\"id\"], \"summary\": row[SUMMARY_COLUMN]} for row in results]\n",
        "        logger.info(\"Retrieved %d summaries\", len(retrieved))\n",
        "        return retrieved\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error executing similarity query: %s\", str(e))\n",
        "        return []\n",
        "\n",
        "# Test Step 3\n",
        "query = \"If Comcast had served 2.5 million households instead, what would its market share have been?\"\n",
        "top_summaries = retrieve_similar_summaries(query, top_k=5)\n",
        "for summary in top_summaries:\n",
        "    print(f\"ID: {summary['id']}, Summary: {summary['summary']}\")\n",
        "logger.info(\"Step 3 complete: Retrieval implemented.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c21b7d-26c8-4b8a-b4d6-0ed14dd3695c",
      "metadata": {
        "tags": [],
        "id": "25c21b7d-26c8-4b8a-b4d6-0ed14dd3695c",
        "outputId": "e463bc8c-5666-41a9-81f6-3a5009a065bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Generating response for query: If Comcast had served 2.5 million households instead, what would its market share have been?\n",
            "INFO:__main__:Step 4 complete: Generation implemented.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Response: The available data does not include information on Comcast's market share in July 2024 if it had served 2.5 million households.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Step4: Generating Context-Aware Responses Using Gemini 2.0 Flash\n",
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "gen_model = GenerativeModel(\"gemini-2.0-flash\")\n",
        "def generate_response(query, retrieved_summaries):\n",
        "    context = \"\\n\".join([s[\"summary\"] for s in retrieved_summaries])\n",
        "    prompt = f\"\"\"You are a professional data analyst providing concise and accurate answers based on the given context. Follow these rules:\n",
        "1. Answer the query directly in a single, complete sentence, focusing only on the specific information requested (e.g., if the query asks for the number of households, provide the number in a sentence; if it asks for market share, provide the market share in a sentence; if it asks for flow share, provide the flow share in a sentence).\n",
        "2. Do not include extraneous details not directly relevant to the query (e.g., do not include zip code-level data for a state-level query, or market share for a query about households).\n",
        "3. If the requested information is not available in the context, respond with: \"The available data does not include information on [requested information] for [location/time].\"\n",
        "4. Keep the response simple and professional, without emojis, bold text, or unnecessary formatting.\n",
        "5. Use the context provided to extract the most relevant information for the query, ensuring the response includes the time period, location, and provider mentioned in the query.\n",
        "\n",
        "Query: {query}\n",
        "Context: {context}\n",
        "Answer based on the context:\n",
        "\"\"\"\n",
        "    logger.info(f\"Generating response for query: {query}\")\n",
        "    try:\n",
        "        response = gen_model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating response: {e}\")\n",
        "        return \"Error generating response\"\n",
        "\n",
        "# Test Step 4\n",
        "response = generate_response(query, top_summaries)\n",
        "print(f\"Generated Response: {response}\")\n",
        "logger.info(\"Step 4 complete: Generation implemented.\")"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m128",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}